[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a unified authoring platform that makes it easy (for reasonably computer-savvy users) to make academic content in all its forms available on the web in a coherent and easily maintained way, increasing both presence and reach of the work going on in an AI research group or CDT.\nContributions made using the Github repo"
  },
  {
    "objectID": "templates/template_for_content.html#image",
    "href": "templates/template_for_content.html#image",
    "title": "Title for your work",
    "section": "Image",
    "text": "Image"
  },
  {
    "objectID": "templates/template_for_content.html#paper-authors",
    "href": "templates/template_for_content.html#paper-authors",
    "title": "Title for your work",
    "section": "Paper-authors",
    "text": "Paper-authors"
  },
  {
    "objectID": "templates/template_for_content.html#venue",
    "href": "templates/template_for_content.html#venue",
    "title": "Title for your work",
    "section": "Venue",
    "text": "Venue"
  },
  {
    "objectID": "templates/template_for_content.html#video",
    "href": "templates/template_for_content.html#video",
    "title": "Title for your work",
    "section": "Video",
    "text": "Video"
  },
  {
    "objectID": "templates/template_for_content.html#slides",
    "href": "templates/template_for_content.html#slides",
    "title": "Title for your work",
    "section": "Slides",
    "text": "Slides\nhere"
  },
  {
    "objectID": "templates/template_for_content.html#code",
    "href": "templates/template_for_content.html#code",
    "title": "Title for your work",
    "section": "Code",
    "text": "Code\nHere"
  },
  {
    "objectID": "templates/template_for_content.html#more-resources",
    "href": "templates/template_for_content.html#more-resources",
    "title": "Title for your work",
    "section": "More Resources",
    "text": "More Resources\nPDF\nPoster\nSupplement\nScholar\nBlog Post"
  },
  {
    "objectID": "posts/test/index.html",
    "href": "posts/test/index.html",
    "title": "test",
    "section": "",
    "text": "asdf123"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Interactive Content Repository",
    "section": "",
    "text": "test\n\n\n\n\n\n\n\nNatural Language Processing\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Interactive Content Repository",
    "section": "",
    "text": "Trainable Transformer in Transformer\n\n\n\n\n\n\n\ncomputation and language\n\n\n\n\nRecent works attribute the capability of in-context learning (ICL) in largepre-trained language models to implicitly simulating and fine-tuning aninternal model (e.g., linear or 2-layer MLP) during inference. However, suchconstructions requireâ€¦\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts_by_author.html",
    "href": "posts_by_author.html",
    "title": "Interactive Content Repository",
    "section": "",
    "text": "name = {\n        const params = new Proxy(new URLSearchParams(window.location.search), {\n            get: (searchParams, prop) =&gt; searchParams.get(prop),\n        });\n        const name = params.name \n        return name \n    }\n\n\n\n\n\n\n\nhtml`&lt;h1&gt;${name}&lt;/h1&gt;`\n\n\n\n\n\n\n\ndata = FileAttachment(\"input.csv\").csv({ typed: true })\n\nnewData = data.filter(function (item) {\n  return item.authors.includes(name)\n});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlink = {\n    \n    var idx = newData[0].authors.split(\",\").indexOf(name) \n    \n    var link = ''\n    console.log(newData[0].authors_link)\n    if(newData[0].authors_link ){\n\n        link = newData[0].authors_link.split(\",\")[idx]\n    }else{\n        link = ''\n    }\n\n    return link\n\n}\n\ntext = {\n    \n    var idx = newData[0].authors.split(\",\").indexOf(name) \n    \n    var text =  ''\n    if(newData[0].authors_link !== 'NONE'){\n        console.log(newData[0].authors_link.split(\",\")[idx])\n        text  = 'Arxiv link'\n    }else{\n        \n        text = ''\n    }\n\n    return text\n\n}\n\nhtml`&lt;a href=${link}&gt;${text}&lt;/a&gt;`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInputs.table(newData.map(d =&gt; ({publication_set: [[d.publication,d.publication_url]], ...d})), {\n        sort: \"start\",\n        reverse: true, \n        format: {\n            publication_set: links =&gt; htl.html`${links.map((link, i) =&gt; htl.html`&lt;a href=${link[1]} &gt;${link[0]}&lt;/a&gt; `)}`,\n            research_area: item =&gt;  htl.html`&lt;a href='/#category=${item}' &gt;${item}&lt;/a&gt;`\n        },  \n        layout: \"fixed\",      \n        width: {\n            publication_set: 1200,\n        },\n        columns:[\n            \"publication_set\",\n            \"research_area\"\n        ],  \n        header: {\n            publication_set: \"Publication\",\n            research_area: \"Research Area\"\n        },\n        align: {\n            publication_set: \"center\",        \n        }\n    })"
  },
  {
    "objectID": "templates/template_for_blogposts.html",
    "href": "templates/template_for_blogposts.html",
    "title": "Title",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Bibendum neque egestas congue quisque egestas diam in arcu cursus. Eget nunc lobortis mattis aliquam faucibus purus in massa tempor. Sit amet luctus venenatis lectus. Nunc vel risus commodo viverra maecenas accumsan. Viverra nam libero justo laoreet sit. Enim tortor at auctor urna. Sit amet facilisis magna etiam tempor. Sit amet massa vitae tortor condimentum lacinia quis vel. Urna nunc id cursus metus aliquam. Pellentesque adipiscing commodo elit at imperdiet. Arcu felis bibendum ut tristique et egestas. Sit amet nulla facilisi morbi tempus iaculis. Purus ut faucibus pulvinar elementum integer enim. Convallis convallis tellus id interdum velit laoreet id.\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Bibendum neque egestas congue quisque egestas diam in arcu cursus. Eget nunc lobortis mattis aliquam faucibus purus in massa tempor. Sit amet luctus venenatis lectus. Nunc vel risus commodo viverra maecenas accumsan. Viverra nam libero justo laoreet sit. Enim tortor at auctor urna. Sit amet facilisis magna etiam tempor. Sit amet massa vitae tortor condimentum lacinia quis vel. Urna nunc id cursus metus aliquam. Pellentesque adipiscing commodo elit at imperdiet. Arcu felis bibendum ut tristique et egestas. Sit amet nulla facilisi morbi tempus iaculis. Purus ut faucibus pulvinar elementum integer enim. Convallis convallis tellus id interdum velit laoreet id.\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Bibendum neque egestas congue quisque egestas diam in arcu cursus. Eget nunc lobortis mattis aliquam faucibus purus in massa tempor. Sit amet luctus venenatis lectus. Nunc vel risus commodo viverra maecenas accumsan. Viverra nam libero justo laoreet sit. Enim tortor at auctor urna. Sit amet facilisis magna etiam tempor. Sit amet massa vitae tortor condimentum lacinia quis vel. Urna nunc id cursus metus aliquam. Pellentesque adipiscing commodo elit at imperdiet. Arcu felis bibendum ut tristique et egestas. Sit amet nulla facilisi morbi tempus iaculis. Purus ut faucibus pulvinar elementum integer enim. Convallis convallis tellus id interdum velit laoreet id."
  },
  {
    "objectID": "content/trainable-transformer-in-transformer/index.html",
    "href": "content/trainable-transformer-in-transformer/index.html",
    "title": "Trainable Transformer in Transformer",
    "section": "",
    "text": "names = [\"Abhishek Panigrahi\",\"Sadhika Malladi\",\"Mengzhou Xia\",\"Sanjeev Arora\"]"
  },
  {
    "objectID": "content/trainable-transformer-in-transformer/index.html#tldr",
    "href": "content/trainable-transformer-in-transformer/index.html#tldr",
    "title": "Trainable Transformer in Transformer",
    "section": "Tldr",
    "text": "Tldr\nRecent works attribute the capability of in-context learning (ICL) in largepre-trained language models to implicitly simulating and fine-tuning aninternal model (e.g., linear or 2-layer MLP) during inference. However, suchconstructions require large memory overhead, which makes simulation of moresophisticated internal models intractable. In this work, we propose anefficient construction, Transformer in Transformer (in short, TinT), thatallows a transformer to simulate and fine-tune complex models internally duringinference (e.g., pre-trained language models). In particular, we introduceinnovative approximation techniques that allow a TinT model with less than 2billion parameters to simulate and fine-tune a 125 million parametertransformer model within a single forward pass. TinT accommodates many commontransformer variants and its design ideas also improve the efficiency of pastinstantiations of simple models inside transformers. We conduct end-to-endexperiments to validate the internal fine-tuning procedure of TinT on variouslanguage modeling and downstream tasks. For example, even with a limitedone-step budget, we observe TinT for a OPT-125M model improves performance by4-16% absolute on average compared to OPT-125M. These findings suggest thatlarge pre-trained language models are capable of performing intricatesubroutines. To facilitate further work, a modular and extensible codebase forTinT is included."
  },
  {
    "objectID": "content/trainable-transformer-in-transformer/index.html#paper-authors",
    "href": "content/trainable-transformer-in-transformer/index.html#paper-authors",
    "title": "Trainable Transformer in Transformer",
    "section": "Paper-authors",
    "text": "Paper-authors\n\nhtml`&lt;ul&gt;${names.map(name =&gt; html`&lt;li&gt;&lt;a href=\"../../posts_by_author.html?name=${name}\" &gt;${name}&lt;/a&gt;&lt;/li&gt;`)}&lt;/ul&gt;`\n\n\n\n\n\n\n\nhtl = require(\"htl@0.2\")\n\n\n\n\n\n\n\nhtml = htl.html"
  },
  {
    "objectID": "content/trainable-transformer-in-transformer/index.html#more-resources",
    "href": "content/trainable-transformer-in-transformer/index.html#more-resources",
    "title": "Trainable Transformer in Transformer",
    "section": "More Resources",
    "text": "More Resources"
  }
]