---
description: In recent years, NLP practitioners have converged on the following practice:(i)
  import an off-the-shelf pretrained (masked) language model; (ii) append amultilayer
  perceptron atop the CLS token's hidden representation (with randomlyinitialized
  weights); and (iii) fine-tune the entire model on a downstream task(MLP). This procedure
  has produced massive gains on standard NLP benchmarks,but these models remain brittle,
  even to mild adversarial perturbations, suchas word-level synonym substitutions.
  In this work, we demonstrate surprisinggains in adversarial robustness enjoyed by
  Model-tuning Via Prompts (MVP), analternative method of adapting to downstream tasks.
  Rather than modifying themodel (by appending an MLP head), MVP instead modifies
  the input (by appendinga prompt template). Across three classification datasets,
  MVP improvesperformance against adversarial word-level synonym substitutions by
  an averageof 8% over standard methods and even outperforms adversarial training-basedstate-of-art
  defenses by 3.5%. By combining MVP with adversarial training, weachieve further
  improvements in robust accuracy while maintaining cleanaccuracy. Finally, we conduct
  ablations to investigate the mechanism underlyingthese gains. Notably, we find that
  the main causes of vulnerability of MLP canbe attributed to the misalignment between
  pre-training and fine-tuning tasks,and the randomly initialized MLP parameters.
  Code is available atthis https URL
format:
  html:
    df-print: paged
    toc: true
image: https://upload.wikimedia.org/wikipedia/commons/5/59/Empty.png
params:
  author_1:
    name: Raman, Mrigank
  author_2:
    name: Maini, Pratyush
  author_3:
    name: Kolter, J. Zico
  author_4:
    name: Lipton, Zachary C.
  author_5:
    name: Pruthi, Danish
  overview: In recent years, NLP practitioners have converged on the following practice:(i)
    import an off-the-shelf pretrained (masked) language model; (ii) append amultilayer
    perceptron atop the CLS token's hidden representation (with randomlyinitialized
    weights); and (iii) fine-tune the entire model on a downstream task(MLP). This
    procedure has produced massive gains on standard NLP benchmarks,but these models
    remain brittle, even to mild adversarial perturbations, suchas word-level synonym
    substitutions. In this work, we demonstrate surprisinggains in adversarial robustness
    enjoyed by Model-tuning Via Prompts (MVP), analternative method of adapting to
    downstream tasks. Rather than modifying themodel (by appending an MLP head), MVP
    instead modifies the input (by appendinga prompt template). Across three classification
    datasets, MVP improvesperformance against adversarial word-level synonym substitutions
    by an averageof 8% over standard methods and even outperforms adversarial training-basedstate-of-art
    defenses by 3.5%. By combining MVP with adversarial training, weachieve further
    improvements in robust accuracy while maintaining cleanaccuracy. Finally, we conduct
    ablations to investigate the mechanism underlyingthese gains. Notably, we find
    that the main causes of vulnerability of MLP canbe attributed to the misalignment
    between pre-training and fine-tuning tasks,and the randomly initialized MLP parameters.
    Code is available atthis https URL
  pdf_url: http://arxiv.org/pdf/2303.07320
title: Model-tuning Via Prompts Makes NLP Models Adversarially Robust

---
## Tldr 
In recent years, NLP practitioners have converged on the following practice:(i) import an off-the-shelf pretrained (masked) language model; (ii) append amultilayer perceptron atop the CLS token's hidden representation (with randomlyinitialized weights); and (iii) fine-tune the entire model on a downstream task(MLP). This procedure has produced massive gains on standard NLP benchmarks,but these models remain brittle, even to mild adversarial perturbations, suchas word-level synonym substitutions. In this work, we demonstrate surprisinggains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), analternative method of adapting to downstream tasks. Rather than modifying themodel (by appending an MLP head), MVP instead modifies the input (by appendinga prompt template). Across three classification datasets, MVP improvesperformance against adversarial word-level synonym substitutions by an averageof 8% over standard methods and even outperforms adversarial training-basedstate-of-art defenses by 3.5%. By combining MVP with adversarial training, weachieve further improvements in robust accuracy while maintaining cleanaccuracy. Finally, we conduct ablations to investigate the mechanism underlyingthese gains. Notably, we find that the main causes of vulnerability of MLP canbe attributed to the misalignment between pre-training and fine-tuning tasks,and the randomly initialized MLP parameters. Code is available atthis https URL

## Paper-authors
- [{{< meta params.author_1.name >}}]({{< meta params.author_1.url >}})
- [{{< meta params.author_2.name >}}]({{< meta params.author_2.url >}})
- [{{< meta params.author_3.name >}}]({{< meta params.author_3.url >}})
- [{{< meta params.author_4.name >}}]({{< meta params.author_4.url >}})
- [{{< meta params.author_5.name >}}]({{< meta params.author_5.url >}})

## More Resources
[![](https://img.shields.io/badge/PDF-green?style=flat)]({{< meta params.pdf_url >}})
