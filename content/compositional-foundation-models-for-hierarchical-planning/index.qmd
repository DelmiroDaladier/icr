---
categories: machine learning
description: To make effective decisions in novel environments with long-horizon goals,
  itis crucial to engage in hierarchical reasoning across spatial and temporalscales.
  This entails planning abstract subgoal sequences, visually reasoningabout the underlying
  plans, and executing actions in accordance with thedevised plan through visual-motor
  control. We propose Compositional FoundationModels for Hierarchical Planning (HiP),
  a foundation model which leveragesmultiple expert foundation model trained on language,
  vision and action dataindividually jointly together to solve long-horizon tasks.
  We use a largelanguage model to construct symbolic plans that are grounded in the
  environmentthrough a large video diffusion model. Generated video plans are then
  groundedto visual-motor control, through an inverse dynamics model that infers actionsfrom
  generated videos. To enable effective reasoning within this hierarchy, weenforce
  consistency between the models via iterative refinement. We illustratethe efficacy
  and adaptability of our approach in three different long-horizontable-top manipulation
  tasks.
execute:
  echo: false
format:
  html:
    df-print: paged
    toc: true
image: https://upload.wikimedia.org/wikipedia/commons/5/59/Empty.png
params:
  author_1:
    link: https://arxiv.org/find/cs/1/au:+Ajay_A/0/1/0/all/0/1
    name: Ajay, Anurag
  author_10:
    link: https://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1
    name: Agrawal, Pulkit
  author_2:
    link: https://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1
    name: Han, Seungwook
  author_3:
    link: https://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1
    name: Du, Yilun
  author_4:
    link: https://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1
    name: Li, Shaung
  author_5:
    link: https://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1
    name: Gupta, Abhi
  author_6:
    link: https://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1
    name: Jaakkola, Tommi
  author_7:
    link: https://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1
    name: Tenenbaum, Josh
  author_8:
    link: https://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1
    name: Kaelbling, Leslie
  author_9:
    link: https://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1
    name: Srivastava, Akash
  overview: To make effective decisions in novel environments with long-horizon goals,
    itis crucial to engage in hierarchical reasoning across spatial and temporalscales.
    This entails planning abstract subgoal sequences, visually reasoningabout the
    underlying plans, and executing actions in accordance with thedevised plan through
    visual-motor control. We propose Compositional FoundationModels for Hierarchical
    Planning (HiP), a foundation model which leveragesmultiple expert foundation model
    trained on language, vision and action dataindividually jointly together to solve
    long-horizon tasks. We use a largelanguage model to construct symbolic plans that
    are grounded in the environmentthrough a large video diffusion model. Generated
    video plans are then groundedto visual-motor control, through an inverse dynamics
    model that infers actionsfrom generated videos. To enable effective reasoning
    within this hierarchy, weenforce consistency between the models via iterative
    refinement. We illustratethe efficacy and adaptability of our approach in three
    different long-horizontable-top manipulation tasks.
  pdf_url: http://arxiv.org/pdf/2309.08587
  research_area: machine learning
title: Compositional Foundation Models for Hierarchical Planning

---
```{ojs} 

 names = ["Anurag Ajay","Seungwook Han","Yilun Du","Shaung Li","Abhi Gupta","Tommi Jaakkola","Josh Tenenbaum","Leslie Kaelbling","Akash Srivastava","Pulkit Agrawal"] 

``` 

## Tldr 
To make effective decisions in novel environments with long-horizon goals, itis crucial to engage in hierarchical reasoning across spatial and temporalscales. This entails planning abstract subgoal sequences, visually reasoningabout the underlying plans, and executing actions in accordance with thedevised plan through visual-motor control. We propose Compositional FoundationModels for Hierarchical Planning (HiP), a foundation model which leveragesmultiple expert foundation model trained on language, vision and action dataindividually jointly together to solve long-horizon tasks. We use a largelanguage model to construct symbolic plans that are grounded in the environmentthrough a large video diffusion model. Generated video plans are then groundedto visual-motor control, through an inverse dynamics model that infers actionsfrom generated videos. To enable effective reasoning within this hierarchy, weenforce consistency between the models via iterative refinement. We illustratethe efficacy and adaptability of our approach in three different long-horizontable-top manipulation tasks.

## Paper-authors

```{ojs} 

 html`<ul>${names.map(name => html`<li><a href="../../posts_by_author.html?name=${name}" >${name}</a></li>`)}</ul>` 

``` 

```{ojs} 

 htl = require("htl@0.2") 

``` 

```{ojs} 

 html = htl.html 

``` 

## More Resources
[![](https://img.shields.io/badge/PDF-green?style=flat)]({{< meta params.pdf_url >}})
