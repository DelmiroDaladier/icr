---
categories: artificial intelligence
description: Increasing interest in ensuring safety of next-generation ArtificialIntelligence
  (AI) systems calls for novel approaches to embedding morality intoautonomous agents.
  Traditionally, this has been done by imposing explicittop-down rules or hard constraints
  on systems, for example by filtering systemoutputs through pre-defined ethical rules.
  Recently, instead, entirelybottom-up methods for learning implicit preferences from
  human behavior havebecome increasingly popular, such as those for training and fine-tuning
  LargeLanguage Models. In this paper, we provide a systematization of existingapproaches
  to the problem of introducing morality in machines - modeled as acontinuum, and
  argue that the majority of popular techniques lie at theextremes - either being
  fully hard-coded, or entirely learned, where noexplicit statement of any moral principle
  is required. Given the relativestrengths and weaknesses of each type of methodology,
  we argue that more hybridsolutions are needed to create adaptable and robust, yet
  more controllable andinterpretable agents.In particular, we present three case studies
  of recent works which uselearning from experience (i.e., Reinforcement Learning)
  to explicitly providemoral principles to learning agents - either as intrinsic rewards,
  morallogical constraints or textual principles for language models. For example,using
  intrinsic rewards in Social Dilemma games, we demonstrate how it ispossible to represent
  classical moral frameworks for agents. We also present anoverview of the existing
  work in this area in order to provide empiricalevidence for the potential of this
  hybrid approach. We then discuss strategiesfor evaluating the effectiveness of moral
  learning agents. Finally, we presentopen research questions and implications for
  the future of AI safety and ethicswhich are emerging from this framework.
execute:
  echo: false
format:
  html:
    df-print: paged
    toc: true
image: https://upload.wikimedia.org/wikipedia/commons/5/59/Empty.png
params:
  author_1:
    link: https://arxiv.org/find/cs/1/au:+Tennant_E/0/1/0/all/0/1
    name: Tennant, Elizaveta
  author_2:
    link: https://arxiv.org/find/cs/1/au:+Hailes_S/0/1/0/all/0/1
    name: Hailes, Stephen
  author_3:
    link: https://arxiv.org/find/cs/1/au:+Musolesi_M/0/1/0/all/0/1
    name: Musolesi, Mirco
  overview: Increasing interest in ensuring safety of next-generation ArtificialIntelligence
    (AI) systems calls for novel approaches to embedding morality intoautonomous agents.
    Traditionally, this has been done by imposing explicittop-down rules or hard constraints
    on systems, for example by filtering systemoutputs through pre-defined ethical
    rules. Recently, instead, entirelybottom-up methods for learning implicit preferences
    from human behavior havebecome increasingly popular, such as those for training
    and fine-tuning LargeLanguage Models. In this paper, we provide a systematization
    of existingapproaches to the problem of introducing morality in machines - modeled
    as acontinuum, and argue that the majority of popular techniques lie at theextremes
    - either being fully hard-coded, or entirely learned, where noexplicit statement
    of any moral principle is required. Given the relativestrengths and weaknesses
    of each type of methodology, we argue that more hybridsolutions are needed to
    create adaptable and robust, yet more controllable andinterpretable agents.In
    particular, we present three case studies of recent works which uselearning from
    experience (i.e., Reinforcement Learning) to explicitly providemoral principles
    to learning agents - either as intrinsic rewards, morallogical constraints or
    textual principles for language models. For example,using intrinsic rewards in
    Social Dilemma games, we demonstrate how it ispossible to represent classical
    moral frameworks for agents. We also present anoverview of the existing work in
    this area in order to provide empiricalevidence for the potential of this hybrid
    approach. We then discuss strategiesfor evaluating the effectiveness of moral
    learning agents. Finally, we presentopen research questions and implications for
    the future of AI safety and ethicswhich are emerging from this framework.
  pdf_url: http://arxiv.org/pdf/2312.01818
  research_area: artificial intelligence
title: Learning Machine Morality through Experience and Interaction

---
```{ojs} 

 names = ["Elizaveta Tennant","Stephen Hailes","Mirco Musolesi"] 

``` 

## Tldr 
Increasing interest in ensuring safety of next-generation ArtificialIntelligence (AI) systems calls for novel approaches to embedding morality intoautonomous agents. Traditionally, this has been done by imposing explicittop-down rules or hard constraints on systems, for example by filtering systemoutputs through pre-defined ethical rules. Recently, instead, entirelybottom-up methods for learning implicit preferences from human behavior havebecome increasingly popular, such as those for training and fine-tuning LargeLanguage Models. In this paper, we provide a systematization of existingapproaches to the problem of introducing morality in machines - modeled as acontinuum, and argue that the majority of popular techniques lie at theextremes - either being fully hard-coded, or entirely learned, where noexplicit statement of any moral principle is required. Given the relativestrengths and weaknesses of each type of methodology, we argue that more hybridsolutions are needed to create adaptable and robust, yet more controllable andinterpretable agents.In particular, we present three case studies of recent works which uselearning from experience (i.e., Reinforcement Learning) to explicitly providemoral principles to learning agents - either as intrinsic rewards, morallogical constraints or textual principles for language models. For example,using intrinsic rewards in Social Dilemma games, we demonstrate how it ispossible to represent classical moral frameworks for agents. We also present anoverview of the existing work in this area in order to provide empiricalevidence for the potential of this hybrid approach. We then discuss strategiesfor evaluating the effectiveness of moral learning agents. Finally, we presentopen research questions and implications for the future of AI safety and ethicswhich are emerging from this framework.

## Paper-authors

```{ojs} 

 html`<ul>${names.map(name => html`<li><a href="../../posts_by_author.html?name=${name}" >${name}</a></li>`)}</ul>` 

``` 

```{ojs} 

 htl = require("htl@0.2") 

``` 

```{ojs} 

 html = htl.html 

``` 

## More Resources
[![](https://img.shields.io/badge/PDF-green?style=flat)]({{< meta params.pdf_url >}})
