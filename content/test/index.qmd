---
categories:
- item response theory
description: "Automatic speech recognition (ASR) systems should be tested ideally\
  \ using diverse speech test data. A\r\npromising alternative to produce such test\
  \ data is to synthesize speeches from diverse sentences and speakers.\r\nHowever,\
  \ despite the great amount of test data that can be produced, not all speeches are\
  \ equally relevant.\r\nThis paper proposes a two-level Item Response Theory (IRT)\
  \ model to simultaneously evaluate ASR systems,\r\nspeakers and sentences. In the\
  \ first level, the transcription rates obtained by a pool of ASR systems on a set\r\
  \nof synthesized speeches are recorded and then analyzed to estimate: each speech\u2019\
  s difficulty and each ASR\r\nsystem\u2019s ability. In the second level, each speech\u2019\
  s difficulty is decomposed as a function of two factors: the\r\nsentence\u2019s\
  \ difficulty and the speaker\u2019s quality. Thus, the speech\u2019s difficulty\
  \ is high when generated from a\r\ndifficult sentence and a bad speaker, while an\
  \ ASR is good when it is robust to hard speeches. Performed\r\nexperiments revealed\
  \ useful insights on how the quality of speech synthesis and recognition can be\
  \ affected\r\nby distinct factors (e.g., sentence difficulty and speaker ability)."
format:
  html:
    df-print: paged
    toc: true
image: https://upload.wikimedia.org/wikipedia/commons/5/59/Empty.png
params:
  author_1:
    name: Telmo M. Silva Filho
    url: https://scholar.google.com/citations?user=-EGO0o8AAAAJ&hl=en
  author_2:
    name: Chaina S. Oliveira
    url: https://scholar.google.co.uk/scholar?q=chaina+s+oliveira&hl=pt-BR&as_sdt=0&as_vis=1&oi=scholart
  author_3:
    name: "Ricardo Bastos C. Prud\xEAncio"
    url: https://scholar.google.co.uk/citations?user=P6InHKcAAAAJ&hl=pt-BR&oi=sra
  code_url: ''
  overview: "Automatic speech recognition (ASR) systems should be tested ideally using\
    \ diverse speech test data. A\r\npromising alternative to produce such test data\
    \ is to synthesize speeches from diverse sentences and speakers.\r\nHowever, despite\
    \ the great amount of test data that can be produced, not all speeches are equally\
    \ relevant.\r\nThis paper proposes a two-level Item Response Theory (IRT) model\
    \ to simultaneously evaluate ASR systems,\r\nspeakers and sentences. In the first\
    \ level, the transcription rates obtained by a pool of ASR systems on a set\r\n\
    of synthesized speeches are recorded and then analyzed to estimate: each speech\u2019\
    s difficulty and each ASR\r\nsystem\u2019s ability. In the second level, each\
    \ speech\u2019s difficulty is decomposed as a function of two factors: the\r\n\
    sentence\u2019s difficulty and the speaker\u2019s quality. Thus, the speech\u2019\
    s difficulty is high when generated from a\r\ndifficult sentence and a bad speaker,\
    \ while an ASR is good when it is robust to hard speeches. Performed\r\nexperiments\
    \ revealed useful insights on how the quality of speech synthesis and recognition\
    \ can be affected\r\nby distinct factors (e.g., sentence difficulty and speaker\
    \ ability)."
  pdf_url: ''
  poster_url: ''
  scholar_url: ''
  slides_url: ''
  supplement_url: ''
title: A two-level Item Response Theory model to evaluate speech synthesis and recognition

---
## Tldr 
Automatic speech recognition (ASR) systems should be tested ideally using diverse speech test data. A
promising alternative to produce such test data is to synthesize speeches from diverse sentences and speakers.
However, despite the great amount of test data that can be produced, not all speeches are equally relevant.
This paper proposes a two-level Item Response Theory (IRT) model to simultaneously evaluate ASR systems,
speakers and sentences. In the first level, the transcription rates obtained by a pool of ASR systems on a set
of synthesized speeches are recorded and then analyzed to estimate: each speech’s difficulty and each ASR
system’s ability. In the second level, each speech’s difficulty is decomposed as a function of two factors: the
sentence’s difficulty and the speaker’s quality. Thus, the speech’s difficulty is high when generated from a
difficult sentence and a bad speaker, while an ASR is good when it is robust to hard speeches. Performed
experiments revealed useful insights on how the quality of speech synthesis and recognition can be affected
by distinct factors (e.g., sentence difficulty and speaker ability).

## Paper-authors
- [{{< meta params.author_1.name >}}]({{< meta params.author_1.url >}})
- [{{< meta params.author_2.name >}}]({{< meta params.author_2.url >}})
- [{{< meta params.author_3.name >}}]({{< meta params.author_3.url >}})

## More Resources
