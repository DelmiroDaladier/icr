---
categories: artificial intelligence
description: '[Abridged abstract] Large Language Models (LLMs) can solve someundergraduate-level
  to graduate-level physics textbook problems and areproficient at coding. Combining
  these two capabilities could one day enable AIsystems to simulate and predict the
  physical world.We present an evaluation of state-of-the-art (SOTA) LLMs on PhD-level
  toresearch-level computational physics problems. We condition LLM generation onthe
  use of well-documented and widely-used packages to elicit codingcapabilities in
  the physics and astrophysics domains. We contribute $\sim 50$original and challenging
  problems in celestial mechanics (with REBOUND),stellar physics (with MESA), 1D fluid
  dynamics (with Dedalus) and non-lineardynamics (with SciPy). Since our problems
  do not admit unique solutions, weevaluate LLM performance on several soft metrics:
  counts of lines that containdifferent types of errors (coding, physics, necessity
  and sufficiency) as wellas a more "educational" Pass-Fail metric focused on capturing
  the salientphysical ingredients of the problem at hand.As expected, today''s SOTA
  LLM (GPT4) zero-shot fails most of our problems,although about 40\% of the solutions
  could plausibly get a passing grade. About$70-90 \%$ of the code lines produced
  are necessary, sufficient and correct(coding \& physics). Physics and coding errors
  are the most common, with someunnecessary or insufficient lines. We observe significant
  variations acrossproblem class and difficulty. We identify several failure modes
  of GPT4 in thecomputational physics domain.Our reconnaissance work provides a snapshot
  of current computationalcapabilities in classical physics and points to obvious
  improvement targets ifAI systems are ever to reach a basic level of autonomy in
  physics simulationcapabilities.'
execute:
  echo: false
format:
  html:
    df-print: paged
    toc: true
image: https://upload.wikimedia.org/wikipedia/commons/5/59/Empty.png
params:
  author_1:
    link: https://arxiv.org/find/cs/1/au:+Ali_Dib_M/0/1/0/all/0/1
    name: Ali-Dib, Mohamad
  author_2:
    link: https://arxiv.org/find/cs/1/au:+Menou_K/0/1/0/all/0/1
    name: Menou, Kristen
  overview: '[Abridged abstract] Large Language Models (LLMs) can solve someundergraduate-level
    to graduate-level physics textbook problems and areproficient at coding. Combining
    these two capabilities could one day enable AIsystems to simulate and predict
    the physical world.We present an evaluation of state-of-the-art (SOTA) LLMs on
    PhD-level toresearch-level computational physics problems. We condition LLM generation
    onthe use of well-documented and widely-used packages to elicit codingcapabilities
    in the physics and astrophysics domains. We contribute $\sim 50$original and challenging
    problems in celestial mechanics (with REBOUND),stellar physics (with MESA), 1D
    fluid dynamics (with Dedalus) and non-lineardynamics (with SciPy). Since our problems
    do not admit unique solutions, weevaluate LLM performance on several soft metrics:
    counts of lines that containdifferent types of errors (coding, physics, necessity
    and sufficiency) as wellas a more "educational" Pass-Fail metric focused on capturing
    the salientphysical ingredients of the problem at hand.As expected, today''s SOTA
    LLM (GPT4) zero-shot fails most of our problems,although about 40\% of the solutions
    could plausibly get a passing grade. About$70-90 \%$ of the code lines produced
    are necessary, sufficient and correct(coding \& physics). Physics and coding errors
    are the most common, with someunnecessary or insufficient lines. We observe significant
    variations acrossproblem class and difficulty. We identify several failure modes
    of GPT4 in thecomputational physics domain.Our reconnaissance work provides a
    snapshot of current computationalcapabilities in classical physics and points
    to obvious improvement targets ifAI systems are ever to reach a basic level of
    autonomy in physics simulationcapabilities.'
  pdf_url: http://arxiv.org/pdf/2312.02091
  research_area: artificial intelligence
title: Physics simulation capabilities of LLMs

---
```{ojs} 

 names = ["Mohamad Ali-Dib","Kristen Menou"] 

``` 

## Tldr 
[Abridged abstract] Large Language Models (LLMs) can solve someundergraduate-level to graduate-level physics textbook problems and areproficient at coding. Combining these two capabilities could one day enable AIsystems to simulate and predict the physical world.We present an evaluation of state-of-the-art (SOTA) LLMs on PhD-level toresearch-level computational physics problems. We condition LLM generation onthe use of well-documented and widely-used packages to elicit codingcapabilities in the physics and astrophysics domains. We contribute $\sim 50$original and challenging problems in celestial mechanics (with REBOUND),stellar physics (with MESA), 1D fluid dynamics (with Dedalus) and non-lineardynamics (with SciPy). Since our problems do not admit unique solutions, weevaluate LLM performance on several soft metrics: counts of lines that containdifferent types of errors (coding, physics, necessity and sufficiency) as wellas a more "educational" Pass-Fail metric focused on capturing the salientphysical ingredients of the problem at hand.As expected, today's SOTA LLM (GPT4) zero-shot fails most of our problems,although about 40\% of the solutions could plausibly get a passing grade. About$70-90 \%$ of the code lines produced are necessary, sufficient and correct(coding \& physics). Physics and coding errors are the most common, with someunnecessary or insufficient lines. We observe significant variations acrossproblem class and difficulty. We identify several failure modes of GPT4 in thecomputational physics domain.Our reconnaissance work provides a snapshot of current computationalcapabilities in classical physics and points to obvious improvement targets ifAI systems are ever to reach a basic level of autonomy in physics simulationcapabilities.

## Paper-authors

```{ojs} 

 html`<ul>${names.map(name => html`<li><a href="../../posts_by_author.html?name=${name}" >${name}</a></li>`)}</ul>` 

``` 

```{ojs} 

 htl = require("htl@0.2") 

``` 

```{ojs} 

 html = htl.html 

``` 

## More Resources
[![](https://img.shields.io/badge/PDF-green?style=flat)]({{< meta params.pdf_url >}})
