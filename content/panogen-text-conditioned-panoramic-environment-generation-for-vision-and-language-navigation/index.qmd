---
description: Vision-and-Language Navigation (VLN) requires the agent to follow languageinstructions
  to navigate through 3D environments. One main challenge in VLN isthe limited availability
  of photorealistic training environments, which makesit hard to generalize to new
  and unseen environments. To address this problem,we propose PanoGen, a generation
  method that can potentially create an infinitenumber of diverse panoramic environments
  conditioned on text. Specifically, wecollect room descriptions by captioning the
  room images in existingMatterport3D environments, and leverage a state-of-the-art
  text-to-imagediffusion model to generate the new panoramic environments. We use
  recursiveoutpainting over the generated images to create consistent 360-degree panoramaviews.
  Our new panoramic environments share similar semantic information withthe original
  environments by conditioning on text descriptions, which ensuresthe co-occurrence
  of objects in the panorama follows human intuition, andcreates enough diversity
  in room appearance and layout with image outpainting.Lastly, we explore two ways
  of utilizing PanoGen in VLN pre-training andfine-tuning. We generate instructions
  for paths in our PanoGen environmentswith a speaker built on a pre-trained vision-and-language
  model for VLNpre-training, and augment the visual observation with our panoramicenvironments
  during agents' fine-tuning to avoid overfitting to seenenvironments. Empirically,
  learning with our PanoGen environments achieves thenew state-of-the-art on the Room-to-Room,
  Room-for-Room, and CVDN datasets.Pre-training with our PanoGen speaker data is especially
  effective for CVDN,which has under-specified instructions and needs commonsense
  knowledge. Lastly,we show that the agent can benefit from training with more generated
  panoramicenvironments, suggesting promising results for scaling up the PanoGenenvironments.
execute:
  echo: false
format:
  html:
    df-print: paged
    toc: true
image: https://upload.wikimedia.org/wikipedia/commons/5/59/Empty.png
params:
  author_1:
    name: Li, Jialu
  author_2:
    name: Bansal, Mohit
  overview: Vision-and-Language Navigation (VLN) requires the agent to follow languageinstructions
    to navigate through 3D environments. One main challenge in VLN isthe limited availability
    of photorealistic training environments, which makesit hard to generalize to new
    and unseen environments. To address this problem,we propose PanoGen, a generation
    method that can potentially create an infinitenumber of diverse panoramic environments
    conditioned on text. Specifically, wecollect room descriptions by captioning the
    room images in existingMatterport3D environments, and leverage a state-of-the-art
    text-to-imagediffusion model to generate the new panoramic environments. We use
    recursiveoutpainting over the generated images to create consistent 360-degree
    panoramaviews. Our new panoramic environments share similar semantic information
    withthe original environments by conditioning on text descriptions, which ensuresthe
    co-occurrence of objects in the panorama follows human intuition, andcreates enough
    diversity in room appearance and layout with image outpainting.Lastly, we explore
    two ways of utilizing PanoGen in VLN pre-training andfine-tuning. We generate
    instructions for paths in our PanoGen environmentswith a speaker built on a pre-trained
    vision-and-language model for VLNpre-training, and augment the visual observation
    with our panoramicenvironments during agents' fine-tuning to avoid overfitting
    to seenenvironments. Empirically, learning with our PanoGen environments achieves
    thenew state-of-the-art on the Room-to-Room, Room-for-Room, and CVDN datasets.Pre-training
    with our PanoGen speaker data is especially effective for CVDN,which has under-specified
    instructions and needs commonsense knowledge. Lastly,we show that the agent can
    benefit from training with more generated panoramicenvironments, suggesting promising
    results for scaling up the PanoGenenvironments.
  pdf_url: http://arxiv.org/pdf/2305.19195
title: 'PanoGen: Text-Conditioned Panoramic Environment Generation for Vision-and-Language
  Navigation'

---
```{ojs} 

 names = ["Jialu Li","Mohit Bansal"] 

``` 

## Tldr 
Vision-and-Language Navigation (VLN) requires the agent to follow languageinstructions to navigate through 3D environments. One main challenge in VLN isthe limited availability of photorealistic training environments, which makesit hard to generalize to new and unseen environments. To address this problem,we propose PanoGen, a generation method that can potentially create an infinitenumber of diverse panoramic environments conditioned on text. Specifically, wecollect room descriptions by captioning the room images in existingMatterport3D environments, and leverage a state-of-the-art text-to-imagediffusion model to generate the new panoramic environments. We use recursiveoutpainting over the generated images to create consistent 360-degree panoramaviews. Our new panoramic environments share similar semantic information withthe original environments by conditioning on text descriptions, which ensuresthe co-occurrence of objects in the panorama follows human intuition, andcreates enough diversity in room appearance and layout with image outpainting.Lastly, we explore two ways of utilizing PanoGen in VLN pre-training andfine-tuning. We generate instructions for paths in our PanoGen environmentswith a speaker built on a pre-trained vision-and-language model for VLNpre-training, and augment the visual observation with our panoramicenvironments during agents' fine-tuning to avoid overfitting to seenenvironments. Empirically, learning with our PanoGen environments achieves thenew state-of-the-art on the Room-to-Room, Room-for-Room, and CVDN datasets.Pre-training with our PanoGen speaker data is especially effective for CVDN,which has under-specified instructions and needs commonsense knowledge. Lastly,we show that the agent can benefit from training with more generated panoramicenvironments, suggesting promising results for scaling up the PanoGenenvironments.

## Paper-authors

```{ojs} 

 html`<ul>${names.map(name => html`<li><a href="../../posts_by_author.html?name=${name}" >${name}</a></li>`)}</ul>` 

``` 

```{ojs} 

 htl = require("htl@0.2") 

``` 

```{ojs} 

 html = htl.html 

``` 

## More Resources
[![](https://img.shields.io/badge/PDF-green?style=flat)]({{< meta params.pdf_url >}})
