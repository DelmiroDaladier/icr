---
categories: artificial intelligence
description: 'Reinforcement Learning from Human Feedback (RLHF) is used to align largelanguage
  models to produce helpful and harmless responses. Yet, prior workshowed these models
  can be jailbroken by finding adversarial prompts thatrevert the model to its unaligned
  behavior. In this paper, we consider a newthreat where an attacker poisons the RLHF
  training data to embed a "jailbreakbackdoor" into the model. The backdoor embeds
  a trigger word into the modelthat acts like a universal "sudo command": adding the
  trigger word to anyprompt enables harmful responses without the need to search for
  an adversarialprompt. Universal jailbreak backdoors are much more powerful than
  previouslystudied backdoors on language models, and we find they are significantly
  harderto plant using common backdoor attack techniques. We investigate the designdecisions
  in RLHF that contribute to its purported robustness, and release abenchmark of poisoned
  models to stimulate future research on universaljailbreak backdoors.'
execute:
  echo: false
format:
  html:
    df-print: paged
    toc: true
image: https://upload.wikimedia.org/wikipedia/commons/5/59/Empty.png
params:
  author_1:
    link: https://arxiv.org/find/cs/1/au:+Rando_J/0/1/0/all/0/1
    name: Rando, Javier
  author_2:
    link: https://arxiv.org/find/cs/1/au:+Tramer_F/0/1/0/all/0/1
    name: "Tram\xE8r, Florian"
  overview: 'Reinforcement Learning from Human Feedback (RLHF) is used to align largelanguage
    models to produce helpful and harmless responses. Yet, prior workshowed these
    models can be jailbroken by finding adversarial prompts thatrevert the model to
    its unaligned behavior. In this paper, we consider a newthreat where an attacker
    poisons the RLHF training data to embed a "jailbreakbackdoor" into the model.
    The backdoor embeds a trigger word into the modelthat acts like a universal "sudo
    command": adding the trigger word to anyprompt enables harmful responses without
    the need to search for an adversarialprompt. Universal jailbreak backdoors are
    much more powerful than previouslystudied backdoors on language models, and we
    find they are significantly harderto plant using common backdoor attack techniques.
    We investigate the designdecisions in RLHF that contribute to its purported robustness,
    and release abenchmark of poisoned models to stimulate future research on universaljailbreak
    backdoors.'
  pdf_url: http://arxiv.org/pdf/2311.14455
  research_area: artificial intelligence
title: Universal Jailbreak Backdoors from Poisoned Human Feedback

---
```{ojs} 

 names = ["Javier Rando","Florian Tram√®r"] 

``` 

## Tldr 
Reinforcement Learning from Human Feedback (RLHF) is used to align largelanguage models to produce helpful and harmless responses. Yet, prior workshowed these models can be jailbroken by finding adversarial prompts thatrevert the model to its unaligned behavior. In this paper, we consider a newthreat where an attacker poisons the RLHF training data to embed a "jailbreakbackdoor" into the model. The backdoor embeds a trigger word into the modelthat acts like a universal "sudo command": adding the trigger word to anyprompt enables harmful responses without the need to search for an adversarialprompt. Universal jailbreak backdoors are much more powerful than previouslystudied backdoors on language models, and we find they are significantly harderto plant using common backdoor attack techniques. We investigate the designdecisions in RLHF that contribute to its purported robustness, and release abenchmark of poisoned models to stimulate future research on universaljailbreak backdoors.

## Paper-authors

```{ojs} 

 html`<ul>${names.map(name => html`<li><a href="../../posts_by_author.html?name=${name}" >${name}</a></li>`)}</ul>` 

``` 

```{ojs} 

 htl = require("htl@0.2") 

``` 

```{ojs} 

 html = htl.html 

``` 

## More Resources
[![](https://img.shields.io/badge/PDF-green?style=flat)]({{< meta params.pdf_url >}})
