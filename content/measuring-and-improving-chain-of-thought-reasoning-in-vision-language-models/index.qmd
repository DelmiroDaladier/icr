---
categories: computation and language
description: Vision-language models (VLMs) have recently demonstrated strong efficacy
  asvisual assistants that can parse natural queries about the visual content andgenerate
  human-like outputs. In this work, we explore the ability of thesemodels to demonstrate
  human-like reasoning based on the perceived information.To address a crucial concern
  regarding the extent to which their reasoningcapabilities are fully consistent and
  grounded, we also measure the reasoningconsistency of these models. We achieve this
  by proposing a chain-of-thought(CoT) based consistency measure. However, such an
  evaluation requires abenchmark that encompasses both high-level inference and detailed
  reasoningchains, which is costly. We tackle this challenge by proposing aLLM-Human-in-the-Loop
  pipeline, which notably reduces cost while simultaneouslyensuring the generation
  of a high-quality dataset. Based on this pipeline andthe existing coarse-grained
  annotated dataset, we build the CURE benchmark tomeasure both the zero-shot reasoning
  performance and consistency of VLMs. Weevaluate existing state-of-the-art VLMs,
  and find that even the best-performingmodel is unable to demonstrate strong visual
  reasoning capabilities andconsistency, indicating that substantial efforts are required
  to enable VLMs toperform visual reasoning as systematically and consistently as
  humans. As anearly step, we propose a two-stage training framework aimed at improving
  boththe reasoning performance and consistency of VLMs. The first stage involvesemploying
  supervised fine-tuning of VLMs using step-by-step reasoning samplesautomatically
  generated by LLMs. In the second stage, we further augment thetraining process by
  incorporating feedback provided by LLMs to producereasoning chains that are highly
  consistent and grounded. We empiricallyhighlight the effectiveness of our framework
  in both reasoning performance andconsistency.
execute:
  echo: false
format:
  html:
    df-print: paged
    toc: true
image: https://upload.wikimedia.org/wikipedia/commons/5/59/Empty.png
params:
  author_1:
    link: https://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1
    name: Chen, Yangyi
  author_2:
    link: https://arxiv.org/find/cs/1/au:+Sikka_K/0/1/0/all/0/1
    name: Sikka, Karan
  author_3:
    link: https://arxiv.org/find/cs/1/au:+Cogswell_M/0/1/0/all/0/1
    name: Cogswell, Michael
  author_4:
    link: https://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1
    name: Ji, Heng
  author_5:
    link: https://arxiv.org/find/cs/1/au:+Divakaran_A/0/1/0/all/0/1
    name: Divakaran, Ajay
  overview: Vision-language models (VLMs) have recently demonstrated strong efficacy
    asvisual assistants that can parse natural queries about the visual content andgenerate
    human-like outputs. In this work, we explore the ability of thesemodels to demonstrate
    human-like reasoning based on the perceived information.To address a crucial concern
    regarding the extent to which their reasoningcapabilities are fully consistent
    and grounded, we also measure the reasoningconsistency of these models. We achieve
    this by proposing a chain-of-thought(CoT) based consistency measure. However,
    such an evaluation requires abenchmark that encompasses both high-level inference
    and detailed reasoningchains, which is costly. We tackle this challenge by proposing
    aLLM-Human-in-the-Loop pipeline, which notably reduces cost while simultaneouslyensuring
    the generation of a high-quality dataset. Based on this pipeline andthe existing
    coarse-grained annotated dataset, we build the CURE benchmark tomeasure both the
    zero-shot reasoning performance and consistency of VLMs. Weevaluate existing state-of-the-art
    VLMs, and find that even the best-performingmodel is unable to demonstrate strong
    visual reasoning capabilities andconsistency, indicating that substantial efforts
    are required to enable VLMs toperform visual reasoning as systematically and consistently
    as humans. As anearly step, we propose a two-stage training framework aimed at
    improving boththe reasoning performance and consistency of VLMs. The first stage
    involvesemploying supervised fine-tuning of VLMs using step-by-step reasoning
    samplesautomatically generated by LLMs. In the second stage, we further augment
    thetraining process by incorporating feedback provided by LLMs to producereasoning
    chains that are highly consistent and grounded. We empiricallyhighlight the effectiveness
    of our framework in both reasoning performance andconsistency.
  pdf_url: http://arxiv.org/pdf/2309.04461
  research_area: computation and language
title: Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models

---
```{ojs} 

 names = ["Yangyi Chen","Karan Sikka","Michael Cogswell","Heng Ji","Ajay Divakaran"] 

``` 

## Tldr 
Vision-language models (VLMs) have recently demonstrated strong efficacy asvisual assistants that can parse natural queries about the visual content andgenerate human-like outputs. In this work, we explore the ability of thesemodels to demonstrate human-like reasoning based on the perceived information.To address a crucial concern regarding the extent to which their reasoningcapabilities are fully consistent and grounded, we also measure the reasoningconsistency of these models. We achieve this by proposing a chain-of-thought(CoT) based consistency measure. However, such an evaluation requires abenchmark that encompasses both high-level inference and detailed reasoningchains, which is costly. We tackle this challenge by proposing aLLM-Human-in-the-Loop pipeline, which notably reduces cost while simultaneouslyensuring the generation of a high-quality dataset. Based on this pipeline andthe existing coarse-grained annotated dataset, we build the CURE benchmark tomeasure both the zero-shot reasoning performance and consistency of VLMs. Weevaluate existing state-of-the-art VLMs, and find that even the best-performingmodel is unable to demonstrate strong visual reasoning capabilities andconsistency, indicating that substantial efforts are required to enable VLMs toperform visual reasoning as systematically and consistently as humans. As anearly step, we propose a two-stage training framework aimed at improving boththe reasoning performance and consistency of VLMs. The first stage involvesemploying supervised fine-tuning of VLMs using step-by-step reasoning samplesautomatically generated by LLMs. In the second stage, we further augment thetraining process by incorporating feedback provided by LLMs to producereasoning chains that are highly consistent and grounded. We empiricallyhighlight the effectiveness of our framework in both reasoning performance andconsistency.

## Paper-authors

```{ojs} 

 html`<ul>${names.map(name => html`<li><a href="../../posts_by_author.html?name=${name}" >${name}</a></li>`)}</ul>` 

``` 

```{ojs} 

 htl = require("htl@0.2") 

``` 

```{ojs} 

 html = htl.html 

``` 

## More Resources
[![](https://img.shields.io/badge/PDF-green?style=flat)]({{< meta params.pdf_url >}})
