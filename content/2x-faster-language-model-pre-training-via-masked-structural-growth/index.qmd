---
description: 'Acceleration of large language model pre-training is a critical issue
  inpresent NLP research. In this paper, we focus on speeding up pre-training byprogressively
  growing from a small Transformer structure to a large one. Thereare two main research
  problems related to progressive growth: growth scheduleand growth operator. For
  growth schedule, existing work has exploredmulti-stage expansion of depth and feedforward
  layers. However, the impact ofeach dimension on the schedule''s efficiency is still
  an open question. Forgrowth operator, existing work relies on the initialization
  of new weights toinherit knowledge, and achieve only non-strict function preservation,
  limitingfurther optimization of training dynamics. To address these issues, we proposeMasked
  Structural Growth (MSG), including growth schedules involving allpossible dimensions
  and strictly function-preserving growth operators that isindependent of the initialization
  of new weights. Experiments show that MSG issignificantly faster than related work:
  we achieve a speed-up of 80% forBert-base and 120% for Bert-large pre-training.
  Moreover, MSG is able toimprove fine-tuning performances at the same time.'
execute:
  echo: false
format:
  html:
    df-print: paged
    toc: true
image: https://upload.wikimedia.org/wikipedia/commons/5/59/Empty.png
params:
  author_1:
    name: Yao, Yiqun
  author_2:
    name: Zhang, Zheng
  author_3:
    name: Li, Jing
  author_4:
    name: Wang, Yequan
  overview: 'Acceleration of large language model pre-training is a critical issue
    inpresent NLP research. In this paper, we focus on speeding up pre-training byprogressively
    growing from a small Transformer structure to a large one. Thereare two main research
    problems related to progressive growth: growth scheduleand growth operator. For
    growth schedule, existing work has exploredmulti-stage expansion of depth and
    feedforward layers. However, the impact ofeach dimension on the schedule''s efficiency
    is still an open question. Forgrowth operator, existing work relies on the initialization
    of new weights toinherit knowledge, and achieve only non-strict function preservation,
    limitingfurther optimization of training dynamics. To address these issues, we
    proposeMasked Structural Growth (MSG), including growth schedules involving allpossible
    dimensions and strictly function-preserving growth operators that isindependent
    of the initialization of new weights. Experiments show that MSG issignificantly
    faster than related work: we achieve a speed-up of 80% forBert-base and 120% for
    Bert-large pre-training. Moreover, MSG is able toimprove fine-tuning performances
    at the same time.'
  pdf_url: http://arxiv.org/pdf/2305.02869
title: 2x Faster Language Model Pre-training via Masked Structural Growth

---
```{ojs} 

 names = ["Yiqun Yao","Zheng Zhang","Jing Li","Yequan Wang"] 

``` 

## Tldr 
Acceleration of large language model pre-training is a critical issue inpresent NLP research. In this paper, we focus on speeding up pre-training byprogressively growing from a small Transformer structure to a large one. Thereare two main research problems related to progressive growth: growth scheduleand growth operator. For growth schedule, existing work has exploredmulti-stage expansion of depth and feedforward layers. However, the impact ofeach dimension on the schedule's efficiency is still an open question. Forgrowth operator, existing work relies on the initialization of new weights toinherit knowledge, and achieve only non-strict function preservation, limitingfurther optimization of training dynamics. To address these issues, we proposeMasked Structural Growth (MSG), including growth schedules involving allpossible dimensions and strictly function-preserving growth operators that isindependent of the initialization of new weights. Experiments show that MSG issignificantly faster than related work: we achieve a speed-up of 80% forBert-base and 120% for Bert-large pre-training. Moreover, MSG is able toimprove fine-tuning performances at the same time.

## Paper-authors

```{ojs} 

 html`<ul>${names.map(name => html`<li><a href="../../posts_by_author.html?name=${name}" >${name}</a></li>`)}</ul>` 

``` 

```{ojs} 

 htl = require("htl@0.2") 

``` 

```{ojs} 

 html = htl.html 

``` 

## More Resources
[![](https://img.shields.io/badge/PDF-green?style=flat)]({{< meta params.pdf_url >}})
