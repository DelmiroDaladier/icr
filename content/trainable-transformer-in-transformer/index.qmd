---
categories: computation and language
description: Recent works attribute the capability of in-context learning (ICL) in
  largepre-trained language models to implicitly simulating and fine-tuning aninternal
  model (e.g., linear or 2-layer MLP) during inference. However, suchconstructions
  require large memory overhead, which makes simulation of moresophisticated internal
  models intractable. In this work, we propose anefficient construction, Transformer
  in Transformer (in short, TinT), thatallows a transformer to simulate and fine-tune
  complex models internally duringinference (e.g., pre-trained language models). In
  particular, we introduceinnovative approximation techniques that allow a TinT model
  with less than 2billion parameters to simulate and fine-tune a 125 million parametertransformer
  model within a single forward pass. TinT accommodates many commontransformer variants
  and its design ideas also improve the efficiency of pastinstantiations of simple
  models inside transformers. We conduct end-to-endexperiments to validate the internal
  fine-tuning procedure of TinT on variouslanguage modeling and downstream tasks.
  For example, even with a limitedone-step budget, we observe TinT for a OPT-125M
  model improves performance by4-16% absolute on average compared to OPT-125M. These
  findings suggest thatlarge pre-trained language models are capable of performing
  intricatesubroutines. To facilitate further work, a modular and extensible codebase
  forTinT is included.
execute:
  echo: false
format:
  html:
    df-print: paged
    toc: true
image: https://upload.wikimedia.org/wikipedia/commons/5/59/Empty.png
params:
  author_1:
    link: https://arxiv.org/find/cs/1/au:+Panigrahi_A/0/1/0/all/0/1
    name: Panigrahi, Abhishek
  author_2:
    link: https://arxiv.org/find/cs/1/au:+Malladi_S/0/1/0/all/0/1
    name: Malladi, Sadhika
  author_3:
    link: https://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1
    name: Xia, Mengzhou
  author_4:
    link: https://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1
    name: Arora, Sanjeev
  overview: Recent works attribute the capability of in-context learning (ICL) in
    largepre-trained language models to implicitly simulating and fine-tuning aninternal
    model (e.g., linear or 2-layer MLP) during inference. However, suchconstructions
    require large memory overhead, which makes simulation of moresophisticated internal
    models intractable. In this work, we propose anefficient construction, Transformer
    in Transformer (in short, TinT), thatallows a transformer to simulate and fine-tune
    complex models internally duringinference (e.g., pre-trained language models).
    In particular, we introduceinnovative approximation techniques that allow a TinT
    model with less than 2billion parameters to simulate and fine-tune a 125 million
    parametertransformer model within a single forward pass. TinT accommodates many
    commontransformer variants and its design ideas also improve the efficiency of
    pastinstantiations of simple models inside transformers. We conduct end-to-endexperiments
    to validate the internal fine-tuning procedure of TinT on variouslanguage modeling
    and downstream tasks. For example, even with a limitedone-step budget, we observe
    TinT for a OPT-125M model improves performance by4-16% absolute on average compared
    to OPT-125M. These findings suggest thatlarge pre-trained language models are
    capable of performing intricatesubroutines. To facilitate further work, a modular
    and extensible codebase forTinT is included.
  pdf_url: http://arxiv.org/pdf/2307.01189
  research_area: computation and language
title: Trainable Transformer in Transformer

---
```{ojs} 

 names = ["Abhishek Panigrahi","Sadhika Malladi","Mengzhou Xia","Sanjeev Arora"] 

``` 

## Tldr 
Recent works attribute the capability of in-context learning (ICL) in largepre-trained language models to implicitly simulating and fine-tuning aninternal model (e.g., linear or 2-layer MLP) during inference. However, suchconstructions require large memory overhead, which makes simulation of moresophisticated internal models intractable. In this work, we propose anefficient construction, Transformer in Transformer (in short, TinT), thatallows a transformer to simulate and fine-tune complex models internally duringinference (e.g., pre-trained language models). In particular, we introduceinnovative approximation techniques that allow a TinT model with less than 2billion parameters to simulate and fine-tune a 125 million parametertransformer model within a single forward pass. TinT accommodates many commontransformer variants and its design ideas also improve the efficiency of pastinstantiations of simple models inside transformers. We conduct end-to-endexperiments to validate the internal fine-tuning procedure of TinT on variouslanguage modeling and downstream tasks. For example, even with a limitedone-step budget, we observe TinT for a OPT-125M model improves performance by4-16% absolute on average compared to OPT-125M. These findings suggest thatlarge pre-trained language models are capable of performing intricatesubroutines. To facilitate further work, a modular and extensible codebase forTinT is included.

## Paper-authors

```{ojs} 

 html`<ul>${names.map(name => html`<li><a href="../../posts_by_author.html?name=${name}" >${name}</a></li>`)}</ul>` 

``` 

```{ojs} 

 htl = require("htl@0.2") 

``` 

```{ojs} 

 html = htl.html 

``` 

## More Resources
[![](https://img.shields.io/badge/PDF-green?style=flat)]({{< meta params.pdf_url >}})
