---
categories: artificial intelligence
description: Existing work has found that the prompt engineering heavily influences
  theperformance of large language models (LLMs). Chain-of-thought (CoT), as apopular
  prompt engineering technique, prompted LLMs using in-context exampleswith reasoning
  steps. In current studies, the few-shot examples of CoT aregenerally handcrafted
  by humans. However, how the text style of in-contextexamples influence the outputs
  of LLMs still remains under-explored. This paperpresents a novel and effective approach,
  named \textbf{AlignCoT}, to improvethe reasoning capability of LLMs by aligning
  the in-context examples with thenative style of LLMs. ``Native'' refers to the inherent
  characteristic style ofLLMs which can be probed by original zero-shot scenarios.
  AlignCoT isorthogonal to other prompt engineering methods, making it easy to combine
  withstate-of-the-art techniques to further improve the LLMs' performance. Weconduct
  extensive and comprehensive experiments on several benchmarks. Theempirical results
  demonstrate that our AlignCoTsignificantly improvesperformance over the carefully
  handcrafted in-context examples. For instance,with GPT-3.5-turbo, we observed a
  +2.5\% improvement on GSM8K. Furthermore, ourAlignCoT consistently improve the performance
  when combined with otherstate-of-the-art prompt engineering methods. The source
  code and dataset willbe available at\href{https://github.com/yangzhch6/AlignCoT}{this
  https URL}.
execute:
  echo: false
format:
  html:
    df-print: paged
    toc: true
image: https://upload.wikimedia.org/wikipedia/commons/5/59/Empty.png
params:
  author_1:
    link: https://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1
    name: Yang, Zhicheng
  author_2:
    link: https://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1
    name: Wang, Yiwei
  author_3:
    link: https://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1
    name: Huang, Yinya
  author_4:
    link: https://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1
    name: Xiong, Jing
  author_5:
    link: https://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1
    name: Liang, Xiaodan
  author_6:
    link: https://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1
    name: Tang, Jing
  overview: Existing work has found that the prompt engineering heavily influences
    theperformance of large language models (LLMs). Chain-of-thought (CoT), as apopular
    prompt engineering technique, prompted LLMs using in-context exampleswith reasoning
    steps. In current studies, the few-shot examples of CoT aregenerally handcrafted
    by humans. However, how the text style of in-contextexamples influence the outputs
    of LLMs still remains under-explored. This paperpresents a novel and effective
    approach, named \textbf{AlignCoT}, to improvethe reasoning capability of LLMs
    by aligning the in-context examples with thenative style of LLMs. ``Native'' refers
    to the inherent characteristic style ofLLMs which can be probed by original zero-shot
    scenarios. AlignCoT isorthogonal to other prompt engineering methods, making it
    easy to combine withstate-of-the-art techniques to further improve the LLMs' performance.
    Weconduct extensive and comprehensive experiments on several benchmarks. Theempirical
    results demonstrate that our AlignCoTsignificantly improvesperformance over the
    carefully handcrafted in-context examples. For instance,with GPT-3.5-turbo, we
    observed a +2.5\% improvement on GSM8K. Furthermore, ourAlignCoT consistently
    improve the performance when combined with otherstate-of-the-art prompt engineering
    methods. The source code and dataset willbe available at\href{https://github.com/yangzhch6/AlignCoT}{this
    https URL}.
  pdf_url: http://arxiv.org/pdf/2311.13538
  research_area: artificial intelligence
title: 'Speak Like a Native: Prompting Large Language Models in a Native Style'

---
```{ojs} 

 names = ["Zhicheng Yang","Yiwei Wang","Yinya Huang","Jing Xiong","Xiaodan Liang","Jing Tang"] 

``` 

## Tldr 
Existing work has found that the prompt engineering heavily influences theperformance of large language models (LLMs). Chain-of-thought (CoT), as apopular prompt engineering technique, prompted LLMs using in-context exampleswith reasoning steps. In current studies, the few-shot examples of CoT aregenerally handcrafted by humans. However, how the text style of in-contextexamples influence the outputs of LLMs still remains under-explored. This paperpresents a novel and effective approach, named \textbf{AlignCoT}, to improvethe reasoning capability of LLMs by aligning the in-context examples with thenative style of LLMs. ``Native'' refers to the inherent characteristic style ofLLMs which can be probed by original zero-shot scenarios. AlignCoT isorthogonal to other prompt engineering methods, making it easy to combine withstate-of-the-art techniques to further improve the LLMs' performance. Weconduct extensive and comprehensive experiments on several benchmarks. Theempirical results demonstrate that our AlignCoTsignificantly improvesperformance over the carefully handcrafted in-context examples. For instance,with GPT-3.5-turbo, we observed a +2.5\% improvement on GSM8K. Furthermore, ourAlignCoT consistently improve the performance when combined with otherstate-of-the-art prompt engineering methods. The source code and dataset willbe available at\href{https://github.com/yangzhch6/AlignCoT}{this https URL}.

## Paper-authors

```{ojs} 

 html`<ul>${names.map(name => html`<li><a href="../../posts_by_author.html?name=${name}" >${name}</a></li>`)}</ul>` 

``` 

```{ojs} 

 htl = require("htl@0.2") 

``` 

```{ojs} 

 html = htl.html 

``` 

## More Resources
[![](https://img.shields.io/badge/PDF-green?style=flat)]({{< meta params.pdf_url >}})
