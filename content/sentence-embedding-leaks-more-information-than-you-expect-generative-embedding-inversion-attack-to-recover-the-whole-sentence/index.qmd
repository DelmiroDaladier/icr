---
description: Sentence-level representations are beneficial for various natural languageprocessing
  tasks. It is commonly believed that vector representations cancapture rich linguistic
  properties. Currently, large language models (LMs)achieve state-of-the-art performance
  on sentence embedding. However, somerecent works suggest that vector representations
  from LMs can cause informationleakage. In this work, we further investigate the
  information leakage issue andpropose a generative embedding inversion attack (GEIA)
  that aims to reconstructinput sequences based only on their sentence embeddings.
  Given the black-boxaccess to a language model, we treat sentence embeddings as initial
  tokens'representations and train or fine-tune a powerful decoder model to decode
  thewhole sequences directly. We conduct extensive experiments to demonstrate thatour
  generative inversion attack outperforms previous embedding inversionattacks in classification
  metrics and generates coherent and contextuallysimilar sentences as the original
  inputs.
execute:
  echo: false
format:
  html:
    df-print: paged
    toc: true
image: https://upload.wikimedia.org/wikipedia/commons/5/59/Empty.png
params:
  author_1:
    name: Li, Haoran
  author_2:
    name: Xu, Mingshi
  author_3:
    name: Song, Yangqiu
  overview: Sentence-level representations are beneficial for various natural languageprocessing
    tasks. It is commonly believed that vector representations cancapture rich linguistic
    properties. Currently, large language models (LMs)achieve state-of-the-art performance
    on sentence embedding. However, somerecent works suggest that vector representations
    from LMs can cause informationleakage. In this work, we further investigate the
    information leakage issue andpropose a generative embedding inversion attack (GEIA)
    that aims to reconstructinput sequences based only on their sentence embeddings.
    Given the black-boxaccess to a language model, we treat sentence embeddings as
    initial tokens'representations and train or fine-tune a powerful decoder model
    to decode thewhole sequences directly. We conduct extensive experiments to demonstrate
    thatour generative inversion attack outperforms previous embedding inversionattacks
    in classification metrics and generates coherent and contextuallysimilar sentences
    as the original inputs.
  pdf_url: http://arxiv.org/pdf/2305.03010
title: 'Sentence Embedding Leaks More Information than You Expect: Generative Embedding
  Inversion Attack to Recover the Whole Sentence'

---
```{ojs} 

 names = ["Haoran Li","Mingshi Xu","Yangqiu Song"] 

``` 

## Tldr 
Sentence-level representations are beneficial for various natural languageprocessing tasks. It is commonly believed that vector representations cancapture rich linguistic properties. Currently, large language models (LMs)achieve state-of-the-art performance on sentence embedding. However, somerecent works suggest that vector representations from LMs can cause informationleakage. In this work, we further investigate the information leakage issue andpropose a generative embedding inversion attack (GEIA) that aims to reconstructinput sequences based only on their sentence embeddings. Given the black-boxaccess to a language model, we treat sentence embeddings as initial tokens'representations and train or fine-tune a powerful decoder model to decode thewhole sequences directly. We conduct extensive experiments to demonstrate thatour generative inversion attack outperforms previous embedding inversionattacks in classification metrics and generates coherent and contextuallysimilar sentences as the original inputs.

## Paper-authors

```{ojs} 

 html`<ul>${names.map(name => html`<li><a href="../../posts_by_author.html?name=${name}" >${name}</a></li>`)}</ul>` 

``` 

```{ojs} 

 htl = require("htl@0.2") 

``` 

```{ojs} 

 html = htl.html 

``` 

## More Resources
[![](https://img.shields.io/badge/PDF-green?style=flat)]({{< meta params.pdf_url >}})
