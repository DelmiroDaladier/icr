---
categories: artificial intelligence
description: In this paper, we present Jellyfish, an open-source LLM as a universal
  tasksolver for DP. Built on the Llama 2 13B model, Jellyfish is instruction-tunedwith
  the datasets of several typical DP tasks including error detection, dataimputation,
  schema matching, and entity matching, and delivers generalizabilityto other tasks.
  Remarkably, Jellyfish can operate on a local, single, andlow-priced GPU with its
  13 billion parameters, ensuring data security andenabling further tuning. Its proficiency
  in understanding natural languageallows users to manually craft instructions for
  DP tasks. Unlike many existingmethods that heavily rely on prior knowledge, Jellyfish
  acquires domainknowledge during its tuning process and integrates optional knowledge
  injectionduring inference. A distinctive feature of Jellyfish is its interpreter,
  whichelucidates its output decisions. To construct Jellyfish, we develop a series
  ofpre-tuning and DP-tuning techniques. Jellyfish is equipped with an instanceserializer,
  which automatically translates raw data into model prompts, and aknowledge injector,
  which optionally introduces task- and dataset-specificknowledge to enhance DP performance.
  Our evaluation of Jellyfish, using a rangeof real datasets, shows its competitiveness
  compared to state-of-the-artmethods and its strong generalizability to unseen tasks.
  Jellyfish'sperformance rivals that of GPT series models, and its interpreter offersenhanced
  reasoning capabilities compared to GPT-3.5. Furthermore, ourevaluation highlights
  the effectiveness of the techniques employed inconstructing Jellyfish. Our model
  is available at Hugging Face:this https URL .
execute:
  echo: false
format:
  html:
    df-print: paged
    toc: true
image: https://upload.wikimedia.org/wikipedia/commons/5/59/Empty.png
params:
  author_1:
    link: https://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1
    name: Zhang, Haochen
  author_2:
    link: https://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1
    name: Dong, Yuyang
  author_3:
    link: https://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1
    name: Xiao, Chuan
  author_4:
    link: https://arxiv.org/find/cs/1/au:+Oyamada_M/0/1/0/all/0/1
    name: Oyamada, Masafumi
  overview: In this paper, we present Jellyfish, an open-source LLM as a universal
    tasksolver for DP. Built on the Llama 2 13B model, Jellyfish is instruction-tunedwith
    the datasets of several typical DP tasks including error detection, dataimputation,
    schema matching, and entity matching, and delivers generalizabilityto other tasks.
    Remarkably, Jellyfish can operate on a local, single, andlow-priced GPU with its
    13 billion parameters, ensuring data security andenabling further tuning. Its
    proficiency in understanding natural languageallows users to manually craft instructions
    for DP tasks. Unlike many existingmethods that heavily rely on prior knowledge,
    Jellyfish acquires domainknowledge during its tuning process and integrates optional
    knowledge injectionduring inference. A distinctive feature of Jellyfish is its
    interpreter, whichelucidates its output decisions. To construct Jellyfish, we
    develop a series ofpre-tuning and DP-tuning techniques. Jellyfish is equipped
    with an instanceserializer, which automatically translates raw data into model
    prompts, and aknowledge injector, which optionally introduces task- and dataset-specificknowledge
    to enhance DP performance. Our evaluation of Jellyfish, using a rangeof real datasets,
    shows its competitiveness compared to state-of-the-artmethods and its strong generalizability
    to unseen tasks. Jellyfish'sperformance rivals that of GPT series models, and
    its interpreter offersenhanced reasoning capabilities compared to GPT-3.5. Furthermore,
    ourevaluation highlights the effectiveness of the techniques employed inconstructing
    Jellyfish. Our model is available at Hugging Face:this https URL .
  pdf_url: http://arxiv.org/pdf/2312.01678
  research_area: artificial intelligence
title: 'Jellyfish: A Large Language Model for Data Preprocessing'

---
```{ojs} 

 names = ["Haochen Zhang","Yuyang Dong","Chuan Xiao","Masafumi Oyamada"] 

``` 

## Tldr 
In this paper, we present Jellyfish, an open-source LLM as a universal tasksolver for DP. Built on the Llama 2 13B model, Jellyfish is instruction-tunedwith the datasets of several typical DP tasks including error detection, dataimputation, schema matching, and entity matching, and delivers generalizabilityto other tasks. Remarkably, Jellyfish can operate on a local, single, andlow-priced GPU with its 13 billion parameters, ensuring data security andenabling further tuning. Its proficiency in understanding natural languageallows users to manually craft instructions for DP tasks. Unlike many existingmethods that heavily rely on prior knowledge, Jellyfish acquires domainknowledge during its tuning process and integrates optional knowledge injectionduring inference. A distinctive feature of Jellyfish is its interpreter, whichelucidates its output decisions. To construct Jellyfish, we develop a series ofpre-tuning and DP-tuning techniques. Jellyfish is equipped with an instanceserializer, which automatically translates raw data into model prompts, and aknowledge injector, which optionally introduces task- and dataset-specificknowledge to enhance DP performance. Our evaluation of Jellyfish, using a rangeof real datasets, shows its competitiveness compared to state-of-the-artmethods and its strong generalizability to unseen tasks. Jellyfish'sperformance rivals that of GPT series models, and its interpreter offersenhanced reasoning capabilities compared to GPT-3.5. Furthermore, ourevaluation highlights the effectiveness of the techniques employed inconstructing Jellyfish. Our model is available at Hugging Face:this https URL .

## Paper-authors

```{ojs} 

 html`<ul>${names.map(name => html`<li><a href="../../posts_by_author.html?name=${name}" >${name}</a></li>`)}</ul>` 

``` 

```{ojs} 

 htl = require("htl@0.2") 

``` 

```{ojs} 

 html = htl.html 

``` 

## More Resources
[![](https://img.shields.io/badge/PDF-green?style=flat)]({{< meta params.pdf_url >}})
